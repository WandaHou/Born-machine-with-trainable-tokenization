 {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Quantum-Inspired Matrix Product States (MPS)\n",
    "\n",
    "This notebook demonstrates how to use the Quantum-inspired MPS implementation for sequence modeling tasks.\n",
    "\n",
    "## Table of Contents\n",
    "1. Setup and Basic Usage\n",
    "2. Quantum Embeddings\n",
    "3. Training an MPS Model\n",
    "4. Sampling and Generation\n",
    "5. Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from main import MPS, QuantumEmbed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Basic Usage\n",
    "\n",
    "Let's start by creating a simple MPS model and exploring its basic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize a small MPS model\n",
    "mps = MPS(\n",
    "    n=4,              # number of sites (sequence length)\n",
    "    phy_dim=2,        # physical dimension\n",
    "    bond_dim=8,       # bond dimension\n",
    "    voc_size=6        # vocabulary size\n",
    ")\n",
    "\n",
    "# Print model structure\n",
    "print(\"MPS Model Structure:\")\n",
    "print(f\"Number of sites: {mps.n}\")\n",
    "print(f\"Physical dimension: {mps.phy_dim}\")\n",
    "print(f\"Bond dimension: {mps.bond_dim}\")\n",
    "print(f\"Vocabulary size: {mps.emb.v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quantum Embeddings\n",
    "\n",
    "The `QuantumEmbed` class converts discrete tokens into quantum measurements. Let's explore how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a quantum embedding layer\n",
    "quantum_embed = QuantumEmbed(dim=2, voc_size=6, n=4)\n",
    "\n",
    "# Convert some tokens to quantum measurements\n",
    "tokens = [0, 1, 2]\n",
    "measurements = quantum_embed(tokens)\n",
    "\n",
    "print(\"Quantum Measurements:\")\n",
    "print(f\"Input tokens: {tokens}\")\n",
    "print(f\"Measurement shape: {measurements.shape}\")\n",
    "print(\"\\nFirst measurement operator:\")\n",
    "print(measurements[0].real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training an MPS Model\n",
    "\n",
    "Now let's see how to train the MPS model on some example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch.optim as optim\n",
    "from main import iso_op\n",
    "\n",
    "# Create synthetic training data\n",
    "num_samples = 100\n",
    "seq_length = 4\n",
    "training_data = torch.randint(0, 6, (num_samples, seq_length))\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = MPS(n=seq_length, phy_dim=2, bond_dim=8, voc_size=6)\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.mps_blocks, 'name': 'iso'},\n",
    "    {'params': model.emb.parameters(), 'name': 'emb'}\n",
    "], lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for sequence in training_data:\n",
    "        prob = model.prob(sequence.tolist())\n",
    "        loss = -torch.log(prob.abs() + 1e-10)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        iso_op(optimizer, loss)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(training_data)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sampling and Generation\n",
    "\n",
    "After training, we can use the model to generate new sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate multiple samples\n",
    "num_samples = 5\n",
    "generated_samples = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    sequence, prob = model.sample()\n",
    "    generated_samples.append((sequence, prob.item()))\n",
    "    print(f\"Sample {i+1}: {sequence}, Probability: {prob.item():.4f}\")\n",
    "\n",
    "# Demonstrate step-by-step sampling\n",
    "print(\"\\nStep-by-step sampling:\")\n",
    "T = None\n",
    "sequence = []\n",
    "for site in range(model.n):\n",
    "    token, T = model.sample_step(T, site)\n",
    "    sequence.append(token)\n",
    "    print(f\"Step {site+1}: Generated token {token}\")\n",
    "print(f\"Final sequence: {sequence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Features\n",
    "\n",
    "Let's explore some advanced features of the MPS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Examine the full tensor representation (warning: exponential complexity)\n",
    "small_mps = MPS(n=3, phy_dim=2, bond_dim=4, voc_size=6)  # Using small n for demonstration\n",
    "full_state = small_mps.full_tensor\n",
    "print(f\"Full state tensor shape: {full_state.shape}\")\n",
    "\n",
    "# Demonstrate one-hot embedding\n",
    "quantum_embed.reset(one_hot=True)\n",
    "one_hot_measurements = quantum_embed([0, 1, 2])\n",
    "print(f\"\\nOne-hot measurement shape: {one_hot_measurements.shape}\")\n",
    "\n",
    "# Show probability calculation for different sequence lengths\n",
    "print(\"\\nProbability calculations:\")\n",
    "sequences = [\n",
    "    [0],\n",
    "    [0, 1],\n",
    "    [0, 1, 2]\n",
    "]\n",
    "\n",
    "for seq in sequences:\n",
    "    prob = small_mps.prob(seq)\n",
    "    print(f\"Sequence {seq}: probability shape {prob.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial demonstrated the key features of the Quantum-inspired MPS implementation:\n",
    "- Basic model setup and configuration\n",
    "- Quantum embeddings for discrete tokens\n",
    "- Training procedure with isometric constraints\n",
    "- Sequence generation and sampling\n",
    "- Advanced features and probability calculations\n",
    "\n",
    "The MPS model provides an efficient way to represent and generate sequences while maintaining quantum-inspired properties through its tensor network structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}